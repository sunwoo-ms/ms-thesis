\chapter{Methodology}

\section{Context}

\subsection{Course Design}

Our study was conducted alongside a hybrid offering of a computer science course at a large public research-intensive university in the United States. CSE 100/100R is an advanced data structures course that assumes that students are familiar with object-oriented programming, combinatorics, basic probability theory, and computer organization as its prerequisite background and covers topics such as trees, graphs, priority queues, and hash tables. It is primarily taken by sophomore and junior undergraduate students and is core to the curriculums for several majors: all undergraduate students in the Department of Computer Science and Engineering must enroll in the course to fulfill one of the requirements of their degree. The offering that took place in the Fall of 2022 marked the first time this course offered in-person and remote modalities simultaneously.

The course followed a flipped classroom model: throughout a given week, students were expected to watch brief topic videos (each being five to ten minutes in duration) through the LMS for the course, EdStem, and to then attend the instructor-led lectures having already familiarized themselves with the concepts that will be discussed. Each topic video covered a single topic that can also be read about in the online textbook for the course, which itself allowed readers to engage with the content through multiple choice, short answer, and short coding problems that provide them with immediate adaptive feedback \cite{moshiri2016data}. As a consequence, the lectures were not designed to introduce concepts as lectures in other courses do: they focused instead on synthesizing the key points covered in the topic videos, resolving misconceptions by working through examples, and connecting the course content to a real-world context. Similar in function to the lectures were the discussion sections, which provided students with opportunities to reinforce their knowledge of the material and learn more about the course assignments. However, discussion sections were led by the course's teaching assistants (TAs) rather than the instructor, and they were held weekly rather than three times a week. Far fewer students attended the discussion sections than those who attended lectures.

The course also followed the HyFlex model, meaning that lectures and discussion sections were held across three primary modalities that any enrolled student could access:

\begin{itemize}
  \item Students could attend \textbf{in person} at the on-campus lecture hall from which the instructor hosted the sessions in all three modalities.
  \item Students could attend \textbf{remotely} through an online meeting on the conferencing app Zoom, the link to which was provided on all course platforms.
  \item Students could attend \textbf{asynchronously} by watching the video recordings of the sessions that were uploaded to EdStem shortly after they were held.
\end{itemize}

The course took place over a period of 10 weeks, with midterm and final exams being distributed at the end of Weeks 5 and 10, respectively. Lectures were held at 9:00 in the morning weekly on Mondays, Wednesdays, and Fridays. Programming Assignments (PAs) were released each Wednesday morning and due on the following Tuesday night. Project 1 was released on the Wednesday following PA 6 and due two weeks later, while Project 2 was due one week and a half after its release. Almost all lectures were simultaneously held in-person and remotely throughout the quarter. The only exception was the first lecture of Week 2, which was held entirely remotely due to the instructor failing the campus's COVID-19 symptom screening tool. In addition, at the beginning of Week 8, a TA union strike began that restricted students' movement on campus, which may have been an additional obstacle for students who would have otherwise attended lectures in person.

\subsection{Methods of Engagement}

Although course materials and assignments were distributed via EdStem for all students, the single point of difference between them — their respective modalities of attendance — caused the experiences of the course to vary wildly from student to student. Some students chose to attend lectures in person as often as possible, while others chose to never set foot on campus. Certain individuals could go in person to attend the discussion sections (which were also offered across all three modalities), while others opted to watch the discussion video recordings instead. Of the reasons why we decided to study this course, the modality of attendance being the only notable difference between students had the largest influence on our study design: since we could not assume that students would follow the modality corresponding to their section of enrollment, we chose to focus on how students engaged with the course irrespective of their enrollment status.

Students were also able to receive help from the instructional staff at tutor lab hours, which were held daily from 9:00 a.m. to 6:00 p.m., and TA office hours, which were held in hour-long blocks on every weekday. These resources were maintained so that students would be able to receive synchronous help on the assignments at almost any point throughout the week, and given that both were held entirely remotely, students of all modalities could access them from any location. This also meant that the lectures and discussion sections were the only aspects of the course that could be attended in person.

\subsection{Enrollment Counts}

Students who enrolled in the course were prompted to select one of two sections: the first was designated as an in-person course (CSE 100), while the second was designated as a remote course (CSE 100R). The decision to divide the course into two sections was purely administrative in nature, and all students across both sections were allowed to interact with the course however they saw fit. Enrollment counts and waitlist counts for each section were tracked throughout the course's inital enrollment period; by the end, 448 students were enrolled in the course overall, with 190 students enrolled in the in-person section and 258 students enrolled in the remote section.

\section{Data Collection}

\subsection{Surveys}

Over the duration of the course, we distributed three surveys to students. The first survey was sent out at the start of the first week of the course; the second was sent out at the start of the sixth week, directly after the midterm exam; and the third was sent out towards the end of the tenth and final week of the course. The pre-course survey gathered the demographics of students, and it asked them about both their section of enrollment and their personal preference between in-person and remote learning. The mid-course and end-of-course surveys asked students for the approximate number of times that they attended lectures in person, as well as the factors that had enabled or prevented them from attending in person. The final two surveys also asked students to estimate how frequently they had made use of various course resources, including the lecture recordings. The mid-course survey was distributed halfway through the course, so it asked students to provide their estimates for Weeks 1 through 5, while the end-of-course survey asked students to estimate these metrics for Weeks 6 through 10. Students were incentivized to complete the end-of-course survey with the promise that they would receive a small amount of extra credit if the course achieved a high overall survey completion rate, but the surveys were entirely optional, and students were given the ability to opt out of the data collection for this study at the end of each survey.

From the 448 students who were enrolled in the course during our data collection phase, we received 287 responses to the pre-course survey, 144 responses to the mid-course survey, and 232 responses to the end-of-course survey. The full list of questions used in these surveys can be found in Appendices A, B, and C.

\subsection{Follow-Up Interviews}

In the mid-course survey, we asked students to state whether they would be willing to participate in a follow-up interview on their experiences with the course. Of the 40 students who indicated that they were interested, we were able to schedule and conduct interviews with a total of five students. All five interviews took place during the final week of the course, so we were able to ask students questions related to their responses to the pre-course and mid-course surveys.

The interviews were semi-structured. They were designed to allow interviewees to openly share their thoughts going into the course and explain how their experiences within the course shaped how they interacted with it. The full list of prompts used to start these discussions can be found in Appendix D.

All interviews were held over the remote conferencing app Zoom for roughly 30 minutes. Zoom was also used to record and transcribe these interviews for later analysis.

\subsection{Course Analytics}

% Due to the ubiquitous presence of EdStem in the overall structure of the course, we were able to use the platform’s metrics to gather a wide range of information: the distribution of grades for all programming assignments, the times at which students were able to obtain 100 percent completion on these assignments, and even their scores on the midterm and final exams were all automatically gathered through EdStem. We would then go on to match these analytics with survey responses and anonymize them, providing us with a comprehensive dataset to analyze and interpret later.

% In addition to EdStem,

Zoom was used to accurately track remote lecture attendance throughout the course. Matching this remote attendance data with the survey responses enabled us to form five distinct classifications for the types of lecture attendees:

\begin{itemize}
  \item Students who attended an average of at least once a week in person and an average of at least once a week remotely were considered to be \textbf{hybrid attendees}.
  \item Students who attended an average of at least once a week in person and an average of less than once a week remotely were considered to be \textbf{in-person attendees}.
  \item Students who attended an average of less than once a week in person and an average of at least once a week remotely were considered to be \textbf{remote attendees}.
  \item Students who attended an average of less than once a week in person and an average of less than once a week remotely, but made use of the lecture recordings at least once a week on average, were considered to be \textbf{asynchronous attendees}. (This also meant that the previous three classifications were considered, collectively, to be \textbf{synchronous attendees}.)
  \item Students who did not fall into any of the above classifications were considered to be \textbf{non-attendees}.
\end{itemize}

Since in-person attendance data was split across the mid-course and end-of-course surveys, students were given distinct classifications for their attendance during the first and second halves of the course. As a result, it is possible for a student to have been classified as one attendee type in the first half and as a different type in the second half. The full set of attendance counts across each of the modalities offered by the course, including the counts of non-attendees, can be found in Table \ref{tab:attendance-modalities-tab} (in Results).

Just as we were able to use Zoom to track remote attendance throughout the course, we took pictures of the audience at each in-person lecture to measure in-person attendance over time. These attendance counts, along with the remote attendance counts, can be found in Figure \ref{fig:attendance_over_time}.

This research protocol was reviewed by the UCSD IRB and certified exempt. In addition, all materials used to generate the results of the data analysis can be found here: \url{https://bit.ly/thinking-aloud-survey}.